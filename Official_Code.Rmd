---
title: "Project Code"
output: pdf_document
---

```{r, echo=FALSE, message = FALSE}
library("tidyverse")
library("ISLR")
library("pls")
library("glmnet")
library("RColorBrewer")
library("coefplot")
library("car")
library("psych")
library("ggbiplot")
library("corrplot")
library("tidymodels")
library("tidytext")
library("ggrepel")
```

We will start by just manually looking at a condensed version of the data, sorted in ascending order by math test scores, and write down any trends we see so we know what to look for in the actual analyses.

```{r message = FALSE}
# Arranging test scores (math and reading) from highest to lowest for each state for comparison with other variables (just by looking at data)
test_data = read_csv("https://stacks.stanford.edu/file/druid:db586ns4974/seda_state_poolsub_gcs_4.0.csv")
state_info = read_csv("https://stacks.stanford.edu/file/druid:db586ns4974/seda_cov_state_pool_4.0.csv")

state_info_condensed = state_info %>% 
  select(1:11, 14, 17:26, 47:50)
test_state_data_mth_rla = test_data %>% filter(subcat == "all") %>% 
  select(fips, stateabb, gcs_mn_avg_mth_ol, gcs_mn_avg_rla_ol) %>%
  inner_join(state_info_condensed, by = c("fips", "stateabb")) %>% 
  arrange(gcs_mn_avg_mth_ol)
```

Just some initial trends we are seeing (specifically for math scores, although easy to see that reading is very similar):
* Higher percent of whites (asian?) in state have higher test scores; Higher percent of other races in state have lower test scores
* Higher percent of ECD in state have lower test scores
* More people on free/reduced lunch mean lower test scores
* Lower SES composite in state leads to lower test scores
* Possibly higher hswhtblk/rswhtblk (diversity/segregation measure of schools) means lower scores? (less obvious)

Since math and reading test scores had such similar results in some of our other code files ("Mainly_PCA_Code" which is not what we are using anymore), we will pool math and reading test scores so that we don't have to do all the analyses twice only to get the same results.

```{r message = FALSE}
# Original pooled data to combine
pooled_test_data = read_csv("https://stacks.stanford.edu/file/druid:db586ns4974/seda_state_pool_gcs_4.0.csv")
state_info = read_csv("https://stacks.stanford.edu/file/druid:db586ns4974/seda_cov_state_pool_4.0.csv")
```

We first pool the data and manually select only the variables we specifically intend to study. Note that the SES variables are computed based on several other variables (prop. of adults with a bachelor degree or higher, prop. of households below poverty line, prop. unemployed, prop. households receiving snap benefits, prop. households headed by a single mother, and log median income), so for this linear regression and PCA, we will only use the variables giving the SES rating and not include the variables used in that SES calculation (perhaps after this analyses we can use those for a different analysis)

```{r}
# Condensed pooled data
state_info_condensed = state_info %>% 
  select(1:11, 14, 17:26, 47:50)
test_state_data = pooled_test_data %>% filter(subcat == "all") %>% 
  select(fips, stateabb, gcs_mn_avg_ol) %>%
  inner_join(state_info_condensed, by = c("fips", "stateabb"))
colnames(test_state_data) = c("fips", "stateabb", "AvgTestScore", "Urban", "Suburb", 
                              "Town", "Rural", "PerIndian", "PerAsian", "PerHsp", "PerBlk", 
                              "PerWht", "PerFreeLunch", "TotEnroll", "SegDevWhtBlk", 
                              "SegDevWhtHsp", "SegDevFreeLun", "SegDevECD", "DiffExpWhtBlk", 
                              "DiffExpWhtHsp", "DiffExpFreeLun", "DiffExpECD", "PerECD", 
                              "SES_AvgAll", "SES_AvgWht", "SES_AvgBlk", "SES_AvgHsp")
```

We will now run a linear regression to see what the most significant variables are in calculating test scores and what the associated coefficients are so we can understand how these significant variables affect test score.

```{r}
# lm model for pooled data
lm_model = lm(AvgTestScore ~ ., data = test_state_data[, -c(1,2)])
summary(lm_model)

# Get the average pooled test score for Asians
pooled_test_data %>% filter(subcat == "race",
                            subgroup == "asn",
                            stateabb != "PR") %>% 
  group_by(subcat, subgroup) %>% 
  summarize(overall = mean(gcs_mn_avg_ol))

# Get the overall average pooled test score
pooled_test_data %>% 
  filter(subcat == "all",
         stateabb != "PR") %>% 
  group_by(subcat, subgroup) %>% 
  summarize(overall = mean(gcs_mn_avg_ol))

```

From the linear regression above, we are seeing that every variable is highly insignificant except percent asian which has a negative coefficient (as percent asian increases, average test score decreases). Percent asian is likely significant because the average asian score is 7, compared to the overall average of 5.5. Most states have relatively low asian populations, with 49 states and DC being below 10%. However, HI has a 56.8% asian population and the second lowest asian score of 5, with the next highest being 5.5.

Not too sure why percent asian in particular is the only one significant, but this model is highly unreliable and uninterpretable given the multicollinearity of the data. We can prove that we have severe multicollinearity in the data by using the `vif` function. From `vif(lm_model)` we get the error "there are aliased coefficients in the model" which indicates that there is perfect multicollinearity since we are getting this error and not actual vif values. To see which variables have perfect multicollinearity, we use `alias(lm_model)` and obviously see that rural, urban, suburb, and town are perfectly correlated and so is percent white, percent native american, percent asian, percent hispanic, and percent black which all makes sense since these are proportions which roughly add to be 100%.


To improve this multicollinearity, we use principal component analysis (PCA) so that we can then use the results in another linear regression so we can actually make some interpretations. The first PCA (using the `prcomp` function) is essentially ONLY to find the number of principal components we should use in the `principal` function since this is the function that will be able to do varimax rotation for us.

```{r fig.height=7}
# Unrotated PCA (PRCOMP FUNCTION) (ONLY to find # of principal components)
test_state_PCA = test_state_data %>%
  column_to_rownames("stateabb")
test_state_PCA = test_state_PCA[-52,c(-1,-2)]

pcr_model = prcomp(test_state_PCA, scale=TRUE)
stats::biplot(pcr_model, scale=0, cex=.7)

# Determining number of components using Kaiser's Rule
pcr_model$sdev^2
stats::screeplot(pcr_model, type="line")
```

Kaiser's Rule states that we should keep the components which have eigenvalues greater than 1. Thus we keep the first 5 components since we can see the only the first 5 values in `pcr_model$sdev^2` are greater than 1. The scree plot visualizes this as well since the variance levels out after 5 components so we only want the first 5. We could've instead chosen 11 components since this would explain 95% of the original variance in the data (which is typically ideal), however 11 components is quite a bit to interpret so we decided to go with Kaiser's Rule so we would have less components.

```{r fig.height=7}
# The same PCA but now making it so we can color the plot according to which region the US state is located
us_regions = read.csv("./states.csv")

# Adding the region data to our data for this part
test_state_PCA2 = test_state_data[-52,-1] %>%
  inner_join(select(us_regions, stateabb = State.Code, Region), by = "stateabb")
# Making each state as the row name
test_state_PCA2 = test_state_PCA2 %>% 
  column_to_rownames("stateabb")

# PCA and plot by region for pooled data (all the same results as before but just colored)
pcr_model2 = prcomp(test_state_PCA2[,c(-1,-26)], scale=TRUE)
ggbiplot(pcr_model2, ellipse=TRUE,  labels=rownames(test_state_PCA2), groups=test_state_PCA2[,26]) +
  theme(legend.position="bottom") + ggtitle("Biplot for the First Two Principal Components") + labs(color="US Region:") 
```

In the above figure, small angles between arrows  indicate a strong positive correlation between those variables, right angles indicate a lack of correlation between variables, and large angles (such as 180 degrees)  indicate a strong negative correlation between variables. Therefore, perecd looks to be strongly correlated with perblk in the state; meanwhile these two appear to be strongly negatively correlated with the average socioeconomic status rating of people in a state. Southern states appear to have a higher proportion of economically disadvantaged/black people in the state and a lower average socioeconomic rating. Western or northeast states seem to typically have a higher average socioeconomic rating and a lower proportion of these groups. Similar other interpretations like this can be made based on these angles/region colors.

Now we will use the `principal` function so that we can do further analysis such as varimax rotation. We will first analyze the unrotated data using the principal function and then rotate for better interpretability and also so we can use these results in the linear regression later.

```{r fig.height=7, warning = FALSE}
# UNROTATED PCA and plot for pooled data (PRINCIPAL FUNCTION)
princ_model_unrot = principal(test_state_PCA, nfactors=5, rotate="none")

# Loadings from unrotated PCA
princ_model_unrot$loadings
```
We can see from the `princ_model_unrot$loadings` code that there are not very well defined groups in each principal component (typically you want similarly related variables to be in the same principal component as each other). Therefore, to improve interpretability by hoping getting more distinct principal components and to be able to use this PCA in a linear regression, we use varimax rotation:
```{r fig.height=7, warning = FALSE}
# ROTATED PCA and plot for pooled data (PRINCIPAL FUNCTION)
princ_model_rot = principal(test_state_PCA, nfactors=5, rotate="varimax")

# Loadings from rotated PCA
princ_model_rot$loadings

par(mfrow=c(1,1))
biplot.psych(princ_model_rot, labels=rownames(USArrests))
biplot.psych(princ_model_rot, labels=rownames(test_state_PCA), choose = c(1,2))
biplot.psych(princ_model_rot, labels=rownames(test_state_PCA), choose = c(5,2))
```

We can now see from the code `princ_model_rot$loadings` that there are slightly better defined principal components that consist of more distinct "groups" of variables. For example, the first principal component differs from the others by including more information about the school such as the diversity indexes. The second principal component differs from the rest by narrowing in on race/economically disadvantaged components. The third is a bit of a combination of groups such as rural/urban/etc. composition, some racial aspects, some diversity/school aspects, etc. As so on for the other two principal components.

We now plug in the scores from the rotated PCA above into a linear regression so that we are able to see which principal components contribute significantly to test scores. Using this information, we will then be able to make further analyses by examining from `princ_model_rot$loadings` what variables were included in those significant principal components and how those variables are correlated. Based off of this, we will look at if the corresponding coefficient in the linear regression is positive or negative and thus see how those variables affect test scores.

```{r}
# Plugging in the results from the rotated PCA into a linear regression
princ_lm = lm(AvgTestScore~ princ_model_rot$scores[,1] + princ_model_rot$scores[,2] + princ_model_rot$scores[,3] +
              princ_model_rot$scores[,4] + princ_model_rot$scores[,5], data = test_state_data[-52, -c(1,2)])
summary(princ_lm)

# Again checking for multicollinearity to make sure it's improved
vif(princ_lm)
```

We can see that the maximum VIF value now is 3.17 which indicates that collinearity is no longer a serious issue since values of 5 to 10 are problematic. Additionally, we can see from the linear regression summary output that principal components 2 and 5 are significant, with component 3 being just barely insignificant. 

*INTERPRETATIONS FOR PCA AND LINEAR REGRESSION*
Component 2 from `princ_model_rot$loadings` (most significant component by far): the negative entries are suburb, percent white, and SES all/wht/blk. Meanwhile, these are being contrasted with urban, percent hsp/blk/economically disadvantaged/free lunch, total enrollment, hsecdnec (how segregated or not the ecd are, i think), rswhtblk/rsecdnec (look up), and SES hsp. From the linear regression, the coefficient for this component is negative. This indicates that the negative entries in `princ_model_rot$loadings` are positively correlated to test scores since they have the same sign as the coefficient, while the positive entries are negatively correlated with test scores since they have the opposite sign of the coefficient. This is a pretty significant finding considering our thesis is about how race/socioeconomic status are related and the inequalities associated and how these may be correlated with test scores. Therefore, this finding appears to be giving evidence that the percent of white people is positively correlated with a higher socioeconomic status and a higher test score. Meanwhile, a higher percent of black people is positively correlated with a higher percent of economically disadvantaged people, which are both negatively correlated with SES rating (given the opposite direction of arrows) and test scores. 

Side note: The hs- variables are defined as "the average deviation of each student's school racial (or ecd) diversity from the district-wide racial (or ecd) diversity" (a rating of 0 is no segregation, 1 is complete segregation) while similarly, the rs-- variables represent the difference between two groups' exposure to one of the groups at the school, for example, rsecdnec is the ECD minus non-ECD exposure to ECD students at the school (so rs- and hs- variables have extremely similar scores in the dataset). Therefore, with this PCA and linear regression, we are seeing that percent black/ecd students is positively correlated with the "segregation levels" at the school, which are negatively correlated with test scores. Percent white and SES rating are negatively correlated with these segregation levels. This seems to be a pretty misleading variable (maybe in how it was calculated?) since it is essentially saying that a lower proportion of other races/ECD gives a lower segregation (even though it would seem like this should be a higher segregation since there are less racial/financial groups in the school). Meanwhile, as the proportion of other groups increase, segregation increases in the school. OVERALL, perhaps if this segregation variable related to the actual state and not just the school, then it would be easier to interpret. The only interpretable thing we are seeing is that a higher segregation level is negatively correlated with test score (in other words, as the percent of different races increase in the school, "segregation" is increasing and test scores decreasing).

Component 5 from `princ_model_rot$loadings` (significant but much less than comp. 2): the negative entries are suburb, town, rural, percent hsp/wht, total enrollment, hswhtblk, and rswhthsp. Meanwhile, these are being contrasted with urban, percent indian/asian/blk, and SES all/wht/blk/hsp. From the linear regression, we can see that this component also has a negative coefficient (though this coefficient is almost 0 as it is about -0.04). This indicates that the negative entries in `princ_model_rot$loadings` are slightly positively correlated to test scores since they have the same sign as the coefficient, while the positive entries are slightly negatively correlated with test scores since they have the opposite sign of the coefficient. Therefore, we are seeing in this component that the percent of white/hispanic are positively correlated with more suburb/town/rural schools and with those "segregation" levels which are slightly positively correlated with test scores? Meanwhile urban, percent asian/indian/black, total enrollment, and all SES ratings are all positively correlated with each other, but slightly negatively correlated with test scores? I'm not really sure how to interpret this since the coefficient is as small as it is, so there is such little change in the test score from this component and the results don't really make intuitive sense to me? Confusing since it still has a slightly significant p-value.

From all the above results, it does seem as though there is a connection between race, socioeconomic status, and test scores. Therefore, it may be a good idea to look at what variables were actually used to calculate these SES ratings and then look at how these variables, race variables, and the test scores are correlated:

```{r fig.height=7}
race_diversity_SES = state_info[-52,-c(1,3:7,12:13,15:17,20,22:25,28:30,32:34,36:38,40:42,44:46,48:53,55:57)]
corr_data = pooled_test_data %>% filter(subcat == "all") %>% 
  select(c("stateabb", "gcs_mn_avg_ol")) %>%
  inner_join(race_diversity_SES, by = c("stateabb"))

colnames(corr_data) = c("stateabb", "AvgTestScore", "PerAsian", "PerHsp","PerBlk", "PerWht", "PerFreeLunch", "SegDevWhtBlk","SegDevWhtHsp","SegDevECD", "PerECD","BachAvg","PovertyAvg","UnempAvg","SnapBenAvg","SingleMomAvg","SES_AvgAll","MedIncome")

# Correlation Plot
corrplot(cor(corr_data[,-1]), method="circle", type = "lower",tl.col = "black", tl.srt = 45,
         title = "Correlation Plot for Racial and Socioeconomic Variables", mar=c(0,0,1,0))

# Function to calculate the p-values for the correlations
cor.mtest <- function(mat, ...) {
    mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat<- matrix(NA, n, n)
    diag(p.mat) <- 0
    for (i in 1:(n - 1)) {
        for (j in (i + 1):n) {
            tmp <- cor.test(mat[, i], mat[, j], ...)
            p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
        }
    }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}

# matrix of the p-value of the correlation
p.mat <- cor.mtest(corr_data[,-1])

# Correlation plots with p-value
corrplot(cor(corr_data[,-1]), method="circle", type = "lower",tl.col = "black", tl.srt = 45, 
         p.mat = p.mat, sig.level = 0.01, insig = "blank",
         title = "Significant Correlations Only", mar=c(0,1,1,0))
```

From the correlation plots above, it is very clear how the various races differ in their correlation with socioeconomic factors/test scores and how socioeconomic variables are correlated with the diversity/segregation levels of school. Specifically, we can see that white people and black people drastically differ in correlation to these socioeconomic/test score variables, while it also appears that hispanic people are closer to black people in socioeconomic status, though the correlations for hispanic people are not nearly as extreme (and often insignificant). Asian people also do not have many significant correlations either. Definitely seems like more evidence of major inequalities (especially between white people and black people though) and how these are correlated with test score. 

```{r, warning = FALSE}
# PCA using tidymodels

pooled_test_data = read_csv("https://stacks.stanford.edu/file/druid:db586ns4974/seda_state_pool_gcs_4.0.csv")
state_info = read_csv("https://stacks.stanford.edu/file/druid:db586ns4974/seda_cov_state_pool_4.0.csv")

# Condensed pooled data
state_info_condensed = state_info %>% 
  select(1:11, 14, 17:26, 47:50)

test_state_data = pooled_test_data %>% filter(subcat == "all") %>% 
  select(fips, stateabb, gcs_mn_avg_ol) %>%
  inner_join(state_info_condensed, by = c("fips", "stateabb"))

colnames(test_state_data) = c("fips","stateabb", "AvgTestScore","Urban","Suburb","Town","Rural","PerIndian", 
                              "PerAsian", "PerHsp","PerBlk", "PerWht", "PerFreeLunch","TotEnroll", 
                              "SegDevWhtBlk","SegDevWhtHsp","SegDevFreeLun","SegDevECD","DiffExpWhtBlk","DiffExpWhtHsp",
                              "DiffExpFreeLun","DiffExpECD", "PerECD","SES_AvgAll","SES_AvgWht","SES_AvgBlk","SES_AvgHsp")

# Remove PR since it is missing a lot of racial data
# We won't need fips codes either
test_state_PCA = test_state_data %>% 
  filter(stateabb != "PR") %>%  #& stateabb != "DC") %>% 
  select(-fips)

# Create recipe for the pca
pca_rec <- recipe(~., data = test_state_PCA) %>%
  update_role(stateabb, AvgTestScore, new_role = "id") %>%
  step_normalize(all_predictors()) %>% # Probably don't need this step
  step_pca(all_predictors())

# Actually perform the pca
pca_prep <- prep(pca_rec)

# Calculate sd and variation
sdev <- pca_prep$steps[[2]]$res$sdev

percent_variation <- sdev^2 / sum(sdev^2)

var_df <- data.frame(PC=paste0("PC",1:length(sdev)),
                     var_explained=percent_variation,
                     stringsAsFactors = FALSE)

# Plot the variation explained by each PC
var_df %>%
  mutate(PC = fct_inorder(PC)) %>%
  ggplot(aes(x=PC,y=var_explained))+geom_col()


# Plot the composition of the top 5 PCs
tidied_pca <- tidy(pca_prep, 2)

tidied_pca %>%
  filter(component %in% paste0("PC", 1:5)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(value, terms, fill = terms)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~component, nrow = 1) +
    labs(y = NULL)

# Re-plot, but order by decreasing absolute value
tidied_pca %>%
  filter(component %in% paste0("PC", 1:5)) %>%
  mutate(terms = reorder_within(terms, abs(value), component)) %>% 
ggplot(aes(value, terms, fill = terms)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ component, scales = "free_y", nrow = 1) +
  scale_y_reordered() +
  labs(y = NULL) +
  theme(axis.text = element_text(size = 7))

# Plot the first two PCs and color by the test scores
# Note that pca_prep$template is equivalent to juice(pca_prep)
ggplot(pca_prep$template, aes(x = PC1, y = PC2)) +
  geom_point(aes(col = AvgTestScore), alpha = 0.75, size = 1.5) +
  geom_text_repel(aes(label = stateabb), size = 3) +
  scale_color_viridis_c(option = "magma")

# Plot all prinicpal components against average test score
pca_prep$template %>%  
  pivot_longer(3:7, names_to = "component", values_to = "value") %>% 
ggplot(aes(x = value, y = AvgTestScore)) +
  geom_point(alpha = 0.75, size = 1.5) +
  facet_wrap(.~component)

# PC2 vs. Avg Test Score
ggplot(pca_prep$template) +
  geom_point(aes(x = PC2, y = AvgTestScore), alpha = 0.75, size = 1.5) +
  geom_text_repel(aes(x = PC2, y = AvgTestScore, label = stateabb), size = 3) +
  geom_hline(yintercept = 5.5, alpha = 0.5, col = "red")

# SES_AvgAll vs. Avg Test Score
ggplot(test_state_PCA) +
  geom_point(aes(x = -SES_AvgAll, y = AvgTestScore), alpha = 0.75, size = 1.5) +
  geom_text_repel(aes(x = -SES_AvgAll, y = AvgTestScore, label = stateabb), size = 3) +
  geom_hline(yintercept = 5.5, alpha = 0.5, col = "red")

# Plugging in the results from the rotated PCA into a linear regression
princ_lm1 = lm(AvgTestScore ~ PC1 + PC2 + PC3 + PC4 + PC5, data = juice(pca_prep))
summary(princ_lm1)
```

From the linear model of the PCA, it appears that the first four principal components are fairly significant. 
Plotting all of these principal components against average test scores, it looks as if PC2 is strongly 
negatively correlated with test scores, whereas the other principal components don't appear to be as 
strongly correlated. Note that in PC2, HI appears to be an outlier. This is likely due to PC2's high weighting 
of SES, which HI has an unusually high value for, despite their low test scores. When plotting SES against 
test scores, we can see that the graph is very similar to that of PC2 against test scores.

```{r, warning = FALSE}
test_state_PCA_R = test_state_PCA %>% column_to_rownames("stateabb") %>% 
  select(-AvgTestScore)

princ_model_unrot = principal(test_state_PCA_R, nfactors=5, rotate="none")

# Loadings from unrotated PCA
princ_model_unrot$loadings

# ROTATED PCA and plot for pooled data (PRINCIPAL FUNCTION)
princ_model_rot = principal(test_state_PCA_R, nfactors=5, rotate="varimax", scores = TRUE)

# Plot the composition of the RCs, ordered by decreasing absolute value
as.data.frame(princ_model_rot$loadings[]) %>%
  rownames_to_column("terms") %>% 
  pivot_longer(2:6, names_to = "component", values_to = "value") %>% 
  mutate(terms = reorder_within(terms, abs(value), component)) %>% 
ggplot(aes(value, terms, fill = terms)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ component, scales = "free_y", nrow = 1) +
  scale_y_reordered() +
  labs(y = NULL) +
  theme(axis.text = element_text(size = 7))

# The average score of each state
scores = test_state_data %>% 
  select(stateabb, AvgTestScore)

# Plot two principal components against one another
as.data.frame(princ_model_rot$scores) %>% 
  rownames_to_column("stateabb") %>% 
ggplot(aes(x = RC1, y = RC2)) +
  geom_point(alpha = 0.75, size = 1.5) +
  geom_text_repel(aes(label = stateabb), size = 3)

plot(as.data.frame(princ_model_rot$scores))

# Plot all rotated components against average test score
as.data.frame(princ_model_rot$scores) %>% 
  rownames_to_column("stateabb") %>% 
  left_join(scores, by = "stateabb") %>%
  pivot_longer(2:6, names_to = "component", values_to = "value") %>% 
ggplot(aes(x = value, y = AvgTestScore)) +
  geom_point(alpha = 0.75, size = 1.5) +
  facet_wrap(.~component)

# RC2 is the only component that appears to have any relationship with test score
as.data.frame(princ_model_rot$scores) %>% 
  rownames_to_column("stateabb") %>% 
  left_join(scores, by = "stateabb") %>% 
ggplot(aes(x = RC2, y = AvgTestScore)) +
  geom_point(alpha = 0.75, size = 1.5) +
  geom_text_repel(aes(label = stateabb), size = 3) +
  geom_hline(yintercept = 5.5, alpha = 0.5, col = "red")

# Get the data for PC2 and RC2
pc2 = tidied_pca %>%
  filter(component == "PC2") %>%
  select(-id)

rc2 = as.data.frame(princ_model_rot$loadings[]) %>%
  rownames_to_column("terms") %>% 
  select(terms, RC2) %>% 
  pivot_longer("RC2", names_to = "component", values_to = "value")

# Plot the compositions of the 2nd components

# Easier to compare across
rbind(pc2, rc2) %>% 
ggplot(aes(value, terms, fill = terms)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ component, scales = "free_y", nrow = 1) +
  scale_y_reordered() +
  labs(y = NULL) +
  theme(axis.text = element_text(size = 7)) +
  xlim(-1, 1)

# Easier to compare within
rbind(pc2, rc2) %>% 
  mutate(terms = reorder_within(terms, abs(value), component)) %>%
ggplot(aes(value, terms, fill = terms)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ component, scales = "free_y", nrow = 1) +
  scale_y_reordered() +
  labs(y = NULL) +
  theme(axis.text = element_text(size = 7)) +
  xlim(-1, 1)

# Plugging in the results from the rotated PCA into a linear regression
princ_lm = lm(AvgTestScore~ princ_model_rot$scores[,1] + princ_model_rot$scores[,2] + princ_model_rot$scores[,3] +
              princ_model_rot$scores[,4] + princ_model_rot$scores[,5], data = test_state_data[-52, -c(1,2)])
summary(princ_lm)
```

After applying varimax rotation, the second component is the only significant predictor. This is made very 
evident in the plots of the rotated components against test scores. Looking closer at the second component, 
we can see a much stronger negative correlation. The notable outlier of HI before is now closer to the 
other data. Looking at the composition of RC2 as compared to PC2, we see that SES_AvgAll has less of an 
effect on RC2, with PerECD and PerFreeLunch having more of an overall effect. Notably, PerAsian, which 
had a misleading effect in the linear models before we applied PCA, now has a smaller effect on RC2, which 
likely contributes towards HI's more average value. PerWhite, which had almost no influence in PC2, is very 
significant in RC2.
