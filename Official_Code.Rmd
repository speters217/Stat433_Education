---
title: "Project Code"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ISLR)
library(pls)
library(glmnet)
library(RColorBrewer)
library(coefplot)
library(car)
library(psych)
# library(devtools)
# install_github("vqv/ggbiplot")
library(ggbiplot)
```

```{r}
# Arranging test scores (math and reading) from highest to lowest for each state for comparison with other variables (just by looking at data)
test_data = read_csv("https://stacks.stanford.edu/file/druid:db586ns4974/seda_state_poolsub_gcs_4.0.csv")
state_info = read_csv("https://stacks.stanford.edu/file/druid:db586ns4974/seda_cov_state_pool_4.0.csv")

state_info_condensed = state_info %>% 
  select(c(1:11, 14, 17:26, 47:50))
test_state_data_mth_rla = test_data %>% filter(subcat == "all") %>% 
  select(c("fips", "stateabb", "gcs_mn_avg_mth_ol", "gcs_mn_avg_rla_ol")) %>%
  inner_join(state_info_condensed, by = c("fips", "stateabb")) %>% 
  arrange(gcs_mn_avg_mth_ol)
```
Just by manually looking at the above data, notice the following trends (specifically for math scores, although easy to see that reading is very similar):
* Higher percent of whites (asian?) in state have higher test scores; Higher percent of other races in state have lower test scores
* Higher percent of ECD in state have lower test scores
* More people on free/reduced lunch mean lower test scores
* Lower SES composite in state leads to lower test scores
* Possibly higher hswhtblk/rswhtblk means lower scores? (less obvious)

```{r}
# Original pooled data to combine (since math and reading scores are so similar as seen in "Mainly_PCA_Code")
pooled_test_data = read_csv("https://stacks.stanford.edu/file/druid:db586ns4974/seda_state_pool_gcs_4.0.csv")
state_info = read_csv("https://stacks.stanford.edu/file/druid:db586ns4974/seda_cov_state_pool_4.0.csv")
```

```{r}
# Condensed pooled data
state_info_condensed = state_info %>% 
  select(c(1:11, 14, 17:26, 47:50))
test_state_data = pooled_test_data %>% filter(subcat == "all") %>% 
  select(c("fips", "stateabb", "gcs_mn_avg_ol")) %>%
  inner_join(state_info_condensed, by = c("fips", "stateabb"))
```

```{r}
# lm model for pooled data
lm_model = lm(gcs_mn_avg_ol ~ ., data = test_state_data[, -c(1,2)])
summary(lm_model)

# Checking for multicollinearity since p-values are all highly insignificant
# vif(lm_model)
# alias(lm_model)
```
From `vif(lm_model)` we get the error "there are aliased coefficients in the model" which indicates that there is perfect multicollinearity. To see which variables have perfect multicollinearity, we use `alias(lm_model)` and obviously see that rural, urban, suburb, and town are perfectly correlated and so is percent white, percent native american, percent asian, percent hispanic, and percent black which all makes sense.


To improve this multicollinearity, we use principal component analysis (PCA) to then use in another linear regression. The first PCA is essentially ONLY to find the number of principal components we should use in the principal function since this is the function that will be able to do varimax rotation for us 
```{r fig.height=7}
# Unrotated PCA (PRCOMP FUNCTION) (ONLY to find # of principal components)
test_state_PCA = test_state_data %>%
  column_to_rownames("stateabb")
test_state_PCA = test_state_PCA[-52,c(-1,-2)]

pcr_model = prcomp(test_state_PCA, scale=TRUE)
summary(pcr_model)
biplot(pcr_model, scale=0, cex=.7)

# Determining number of components using Kaiser's Rule
pcr_model$sdev^2
screeplot(pcr_model, type="line")
```
Kaiser's Rule states that we should keep the components which have eigenvalues greater than 1. Thus we keep the first 5 components. The scree plot visualizes this as well since the variance levels out after 5 components so we only want the first 5. We could've chosen 11 components since this would explain 95% of the original variance in the data, however 11 components is quite a bit to interpret so we decided to go with Kaiser's Rule.

```{r fig.height=7}
# The same PCA but now making it so we can color the plot according to which region the US state is located
us_regions = read.csv("/Users/mschladweiler/Documents/Senior/Semester 2/Stat 433/Group Project/states.csv")

# Adding the region data to our data for this part
test_state_PCA2 = test_state_data[-52,-1] %>%
  inner_join(select(us_regions, stateabb = State.Code, Region), by = "stateabb")
# Making each state as the row name
test_state_PCA2 = test_state_PCA2 %>% 
  column_to_rownames("stateabb")

# PCA and plot by region for pooled data (all the same results as before but just colored)
pcr_model2 = prcomp(test_state_PCA2[,c(-1,-26)], scale=TRUE)
ggbiplot(pcr_model2, ellipse=TRUE,  labels=rownames(test_state_PCA2), groups=test_state_PCA2[,26]) +
  theme(legend.position="bottom") + ggtitle("Biplot for the First Two Principal Components")
```

Now we will use the principal function so that we can do further analysis such as varimax rotation. We will first analyze the unrotated data using the principal function and then rotate for better interpretability.
```{r fig.height=7}
# UNROTATED PCA and plot for pooled data (PRINCIPAL FUNCTION)
princ_model_unrot = principal(test_state_PCA, nfactors=5, rotate="none")

# Loadings from unrotated PCA
princ_model_unrot$loadings

# Scores from unrotated PCA
princ_model_unrot$scores 

par(mfrow=c(1,1))
biplot.psych(princ_model_unrot, choose = c(1,2))
```
We can see from the `princ_model_unrot$loadings` code that there are not very well defined groups in each principal component. Therefore, to improve interpretability and to be able to use this PCA in a linear regression we use varimax rotation:
```{r fig.height=7}
# ROTATED PCA and plot for pooled data (PRINCIPAL FUNCTION)
princ_model_rot = principal(test_state_PCA, nfactors=5, rotate="varimax")

# Loadings from rotated PCA
princ_model_rot$loadings

# Scores from rotated PCA
princ_model_rot$scores 

par(mfrow=c(1,1))
biplot.psych(princ_model_rot, labels=rownames(test_state_PCA))
```
We can now see from the code `princ_model_rot$loadings` that there are slightly better defined principal components that consist of more distinct "groups" of variables. For example, the first principal component differs from the others by including more information about the school such as the diversity indexes. The second principal component differs from the rest by narrowing in on race/economically disadvantaged components. The third is a bit of a combination of groups such as rural/urban/etc. composition, some racial aspects, some diversity/school aspects, etc. As so on for the other two principal components.

We now plug in the scores from the rotated PCA above into a linear regression so that we are able to see which principal components contribute significantly to test scores. Using this information, we will then be able to make further analyses by examining from `princ_model_rot$loadings` what variables were included in those significant principal components and how those variables are correlated. Based off of this, we will look at if the corresponding coefficient in the linear regression is positive or negative and thus see how those variables effect test scores.
```{r}
# Plugging in the results from the rotated PCA into a linear regression
princ_lm = lm(gcs_mn_avg_ol~ princ_model_rot$scores[,1] + princ_model_rot$scores[,2] + princ_model_rot$scores[,3] +
              princ_model_rot$scores[,4] + princ_model_rot$scores[,5], data = test_state_data[-52, -c(1,2)])
summary(princ_lm)

# Again checking for multicollinearity to make sure it's improved
vif(princ_lm)
```
We can see that the maximum VIF value now is 3.17 which indicates that collinearity is no longer a serious issue since values of 5 to 10 are problematic. Additionally, we can see from the linear regression summary output that principal components 2 and 5 are significant, with component 3 being just barely insignificant. 

*INTERPRETATIONS FOR PCA AND LINEAR REGRESSION*
Component 2 from `princ_model_rot$loadings`: the negative entries are suburb, percent white, and SES all/wht/blk. Meanwhile, these are being contrasted with urban, percent hsp/blk/economically disadvantaged/free lunch, total enrollment, hsecdnec (how segregated or not the ecd are, i think), rswhtblk/rsecdnec (look up), and SES hsp. From the linear regression, the coefficient for this component is negative. This indicates that the negative entries in `princ_model_rot$loadings` are positively correlated to test scores since they have the same sign as the coefficient, while the positive entries are negatively correlated with test scores since they have the opposite sign of the coefficient.

Component 5 from `princ_model_rot$loadings`: the negative entries are suburb, town, rural, percent hsp/wht, total enrollment, hswhtblk (how segregated or not blk are, i think), and rswhthsp (look up). Meanwhile, these are being contrasted with urban, percent indian/asian/blk, and SES all/wht/blk/hsp. From the linear regression, we can see that this component also has a negative coefficient (though the coefficient is smaller than for component 2). This indicates that the negative entries in `princ_model_rot$loadings` are slightly positively correlated to test scores since they have the same sign as the coefficient, while the positive entries are slightly negatively correlated with test scores since they have the opposite sign of the coefficient.





